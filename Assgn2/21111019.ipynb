{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc519ae4",
   "metadata": {},
   "source": [
    "# Answer 1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "497c951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given gradient descent function:-\n",
    "'''This function returns value of x for which derivative of a function is 'near' to zero'''\n",
    "def gradient_descent(gradient,init_,learn_rate,n_iter=50,tol=1e-06):  #init_ denotes initial value of x \n",
    "    x=init_\n",
    "    for i in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)  # minus sign shows we have to move in opposite direction of gradient\n",
    "        if np.all(np.abs(delta)) <= tol:   # if delta<tolerance then x is already \"near\" minima value of x. by the way, this is simpler and will also work here:- if np.abs(delta) <= tol\n",
    "            break                          \n",
    "        x=x+delta\n",
    "    return round(x*1000)/1000\n",
    "\n",
    "\n",
    "#function to calculate value of y at given x\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0078ce31",
   "metadata": {},
   "source": [
    "$(i) y = x^2 + 3x + 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "f3303b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.75\n"
     ]
    }
   ],
   "source": [
    "# Gradient for y is 2x + 3\n",
    "# Here initial value of x is 4,gradient is a function which returns 2*v+3 where v is argument passed to the function\n",
    "# Also learn rates 0.1,0.2,...,0.8 all are giving correct answer so I took learn rate as 0.8 so that convergence is faster\n",
    "\n",
    "min_x = gradient_descent(gradient=lambda v: 2*v+3, init_=4.0, learn_rate=0.8 ) \n",
    "print(f(min_x))   #f(min_x) is global minima of y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9f799",
   "metadata": {},
   "source": [
    "$(ii) y= x^4 - 3x^2 + 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "a1ba4749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.564\n"
     ]
    }
   ],
   "source": [
    "# Gradient is 4(x^3)-6x+2\n",
    "\n",
    "'''Unlike previous case, here we cannot run gradient descent just one time because our initial chosen x might \n",
    "land us to a local optima rather than global optima because here y has multiple optima'''\n",
    "\n",
    "import random\n",
    "\n",
    "global_minima_x=10000    # assigning initial value of x large so that it gets updated in first iteration of for loop\n",
    "\n",
    "for i in range(40):      #running several times so that error in finding minima is as less as possible\n",
    "    start_x=random.randint(-10,10)  #randomly choose different x's\n",
    "    temp_minima_x=gradient_descent(gradient=lambda v: 4*v*v*v - 6*v + 2, init_=start_x, learn_rate=0.001)   # learn rate is reduced as compared to previous case to prevent overflow while calculating gradient\n",
    "    \n",
    "    if f(global_minima_x)>f(temp_minima_x):\n",
    "        global_minima_x=temp_minima_x      #if value of y at new minima x is lesser than value of y at global minima x then update global minima x \n",
    "\n",
    "        \n",
    "#function to evaluate y at given x    \n",
    "def f(x):\n",
    "    return (x**4 - 3*(x**2) + 2*x)\n",
    "\n",
    "print(global_minima_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c4956",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeecdab5",
   "metadata": {},
   "source": [
    "# Answer 1(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778e14e",
   "metadata": {},
   "source": [
    " The regression line  :- $$ y = ax + b $$\n",
    " $$$$\n",
    " Let, $y_i$  = actual  value  and       $\\overline y_i$ = predicted value   $\\ $    by our regression model\n",
    " $$$$\n",
    " Loss function L  $$= \\frac{1}{n} \\sum_{i=1}^n (y_i - \\overline y_i )^2$$\n",
    " \n",
    " $$$$\n",
    " \n",
    " \n",
    " $$= \\frac{1}{n} \\sum_{i=1}^n (y_i - ax_i -b)^2$$\n",
    " \n",
    " $$$$\n",
    " \n",
    " We will separetly find the value of a and b such that the loss function is minimised. For that, we will take partial dervative w.r.t. a then w.r.t. b. We will call this derivative $D_a$ and $D_b$\n",
    " $$$$\n",
    " $$ D_a= \\frac{1}{n} \\sum_{i=1}^n \\frac{ d}{ da} (y_i - ax_i - b)^2  $$\n",
    " $$ D_a= \\frac{1}{n} \\sum_{i=1}^n  2(y_i - ax_i - b)(-x_i)$$\n",
    " Similarly,\n",
    " $$ D_b= \\frac{1}{n} \\sum_{i=1}^n  2(y_i - ax_i - b)(-1)$$\n",
    " $$$$\n",
    " Now by performing several iterations of gradient descent, we will find value of a and b which minimises the loss function. If we call $a_{opt}$ and $b_{opt}$ as the optimum value of a and b respectively then the line $ y=a_{opt}x + b_{opt} $ will fit our data very well.\n",
    " \n",
    "Since the loss function is convex function(it is a polynomial of degree 2), to perform gradient descent on a and b we can choose any initial value of a and b and we will surely land on the global minima if we take proper value of learning rate and perform well enough number of iterations. Here I will initially choose $a =0$ and $b=0$\n",
    "$$$$\n",
    "a and b will be updated according to the following equation:-\n",
    "$$ a=a-L*D_a$$\n",
    "$$ b=b-L*D_b$$\n",
    "minus sign denotes that we move in opposite direction to that of derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "0aca48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the explanation of this code is given above''' \n",
    "\n",
    "#function to return partial derivative wrt a of loss function \n",
    "def gradient_a(Y,a,X,b):\n",
    "    n=len(Y)  # number of data points\n",
    "    Da=(-2/n)*np.dot((Y - a*X -b),(X))\n",
    "    return Da\n",
    "\n",
    "#function to return partial derivative wrt b of loss function \n",
    "def gradient_b(Y,a,X,b):\n",
    "    n=len(Y)\n",
    "    Db=(-2/n)*np.sum(Y - a*X - b)\n",
    "    return Db\n",
    "\n",
    "\n",
    "def gradient_for_lin_reg(Y,X,tol=1e-10):   #Y is a vector having actual values and X is a vector of x-cord of training data points\n",
    "    L=0.05  # learning rate\n",
    "    a=0\n",
    "    b=0 #initial values of a and b set to 0\n",
    "    for i in range(50):         #the number of times this loop runs and the learning rate have been found by trial-error to minimise time complexity\n",
    "        Da=gradient_a(Y,a,X,b)  #calling function gradient_a\n",
    "        Db=gradient_b(Y,a,X,b)  #calling function gradient_b\n",
    "        delta1=L*Da\n",
    "        delta2=L*Db\n",
    "        a=a-delta1\n",
    "        b=b-delta2\n",
    "        \n",
    "        if (np.abs(delta1)<=tol and np.abs(delta2)<=tol):   # this is used to reduce more iterations if a and b both are already near minima\n",
    "            break\n",
    "        \n",
    "    a=round(a*1000)/1000    #rounding upto 3 decimals\n",
    "    b=round(b*1000)/1000\n",
    "    return [a,b]            # returns a list having the optimum [a,b]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66ed66",
   "metadata": {},
   "source": [
    "## Answer 1(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "c54922df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=0.304 , b=1.978\n"
     ]
    }
   ],
   "source": [
    "#checking the above function of ques 1(b) with artificial data\n",
    "np.random.seed(0)\n",
    "X=2.5*np.random.randn(10000) + 1.5  # array of 10000 values with mean 1.5 and standard deviation 2.5\n",
    "res=1.5*np.random.randn(10000)      # array of 10000 values with mean 0 and standard deviation 1.5\n",
    "Y=2 + 0.3*X + res                   # actual values of Y for above generated X\n",
    "list=gradient_for_lin_reg(Y,X)\n",
    "print(\"a={a} , b={b}\".format(a=list[0],b=list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64cd5d",
   "metadata": {},
   "source": [
    "# Answer 1(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f1450",
   "metadata": {},
   "source": [
    "__Gradient Descent:__ In this, we compute the gradient of whole training data in each iteration of loop. Time complexity per iteration is $O(ND)$ ,where N=number of training data and D=number of features in each training data. In above example, N=10000 and D=2.\n",
    "\n",
    "\n",
    "__Stochastic Gradient Descent:__ In this, we assume data is uniformly distributed and we pick any one training data randomly and compute its gradient. Now, we assume this gradient is the representative of the gradient of whole training data and proceed as we did in gradient descent. In next iteration, we again randomly pick one training data and repeat the process. Because of this, per iteration cost is reduced. Time complexity per iteration is $O(D)$ ,where D=number of features in training data.\n",
    "Remember in simple gradient descent, to find $D_a$ we took the average of all gradients over each training data(this is a costly operation when number of training data is large). \n",
    "\n",
    "__Minibatch SGD:__ This is same as SGD except now rather than computing value of gradient at only one point, we randomly take a batch of training data and take the average of their gradients as the representative of gradient of whole training data. Time complexity per iteration is $O(BD)$ , where B= batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "2b1a644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return partial derivative wrt a of loss function \n",
    "def gradient_MB_a(Y_batch,a,X_batch,b,n):   \n",
    "    Da=(-2/n)*np.dot((Y_batch - a*X_batch -b),(X_batch))\n",
    "    return Da\n",
    "\n",
    "#function to return partial derivative wrt b of loss function \n",
    "def gradient_MB_b(Y_batch,a,X_batch,b,n):\n",
    "    Db=(-2/n)*np.sum(Y_batch - a*X_batch - b)\n",
    "    return Db\n",
    "\n",
    "\n",
    "def gradient_for_lin_reg_MB(Y,X,B,tol=1e-06):   #Y is a vector having actual values and X is a vector of x-cord of training data points\n",
    "    L=0.02  # learning rate\n",
    "    a=0\n",
    "    b=0 #initial values of a and b set to 0\n",
    "    \n",
    "\n",
    "    for i in range(500):         #the number of times this loop runs and the learning rate have been found by trial-error to minimise time complexity\n",
    "        \n",
    "        '''ESSENCE OF MINIBATCH SGD LIES IN THIS PART''' \n",
    "        N=len(Y)  # no. of training data \n",
    "    \n",
    "        arr_rand=np.random.randint(N, size=B)   # generate B random integers between 0 to N-1. These will be used as index of training data to be included in our mini-batch\n",
    "\n",
    "        #creation of our mini-batch\n",
    "        training_data_2D_array=np.array([X,Y])  #combining corresponding x and y cordinates \n",
    "        training_data_2D_array=training_data_2D_array.transpose()\n",
    "        batch_array=np.zeros((B,2))                          # creates an array of rows=B and columns=2. This will store our batch training data\n",
    "\n",
    "        k=0\n",
    "        for i in range(B):\n",
    "            batch_array[k][0]=training_data_2D_array[arr_rand[i]][0]   # array_rand stores indexes of those training data which will be included inour batch\n",
    "            batch_array[k][1]=training_data_2D_array[arr_rand[i]][1]\n",
    "            k=k+1\n",
    "       \n",
    "        X_batch=np.array(batch_array[:,0])                #take out x-cordinate of our training batch. we will use this to call gradient_a and gradient_b functions\n",
    "        Y_batch=batch_array[:,1]                          #similarly take out y-cord of our batch\n",
    "    \n",
    "        #compute the average gradient of our mini-batch\n",
    "        for i in range(B):\n",
    "            Da_batch=gradient_MB_a(Y_batch,a,X_batch,b,B)  #calling function gradient_a\n",
    "            Db_batch=gradient_MB_b(Y_batch,a,X_batch,b,B)  #calling function gradient_b\n",
    "        \n",
    "        '''Minibatch SGD kind of ends here. Now the remaining part is same as that of simple gradient descent algorithm'''\n",
    "         \n",
    "        delta1=L*Da_batch\n",
    "        delta2=L*Db_batch\n",
    "        a=a-delta1\n",
    "        b=b-delta2\n",
    "        \n",
    "        if (np.abs(delta1)<=tol and np.abs(delta2)<=tol):   # this is used to reduce more iterations if a and b both are already near minima\n",
    "            break\n",
    "        \n",
    "    \n",
    "    \n",
    "    a=round(a*1000)/1000    #rounding upto 3 decimals\n",
    "    b=round(b*1000)/1000\n",
    "    return [a,b]            # returns a list having the optimum [a,b]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "269cad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=0.316 , b=2.047\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X=2.5*np.random.randn(10000) + 1.5  # array of 10000 values with mean 1.5 and standard deviation 2.5\n",
    "res=1.5*np.random.randn(10000)      # array of 10000 values with mean 0 and standard deviation 1.5\n",
    "Y=2 + 0.3*X + res                   # actual values of Y for above generated X\n",
    "\n",
    "B=150\n",
    "list_MB=gradient_for_lin_reg_MB(Y,X,B)  # B is batch size\n",
    "print(\"a={a} , b={b}\".format(a=list_MB[0],b=list_MB[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b3df",
   "metadata": {},
   "source": [
    "# Answer 1(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e69de",
   "metadata": {},
   "source": [
    "#### Comparing execution time of simple gradient descent , SGD and minibatch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "2daf1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X=2.5*np.random.randn(10000) + 1.5  # array of 10000 values with mean 1.5 and standard deviation 2.5\n",
    "res=1.5*np.random.randn(10000)      # array of 10000 values with mean 0 and standard deviation 1.5\n",
    "Y=2 + 0.3*X + res                   # actual values of Y for above generated X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "54df5224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04259085655212402\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#time taken by SGD\n",
    "B=1\n",
    "start=time.time()\n",
    "gradient_for_lin_reg_MB(Y,X,B)    # we are calling minibatch SGD with batch size=1 which is nothing but SGD\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "8f04d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009478092193603516\n"
     ]
    }
   ],
   "source": [
    "#time taken by simple Gradient descent\n",
    "start=time.time()\n",
    "gradient_for_lin_reg(Y,X)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "8a96277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.665083408355713\n"
     ]
    }
   ],
   "source": [
    "#time taken by minibatch SGD\n",
    "B=150\n",
    "start=time.time()\n",
    "gradient_for_lin_reg_MB(Y,X,B)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "875df2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=0.169 , b=1.813\n"
     ]
    }
   ],
   "source": [
    "#result produced by SGD\n",
    "B=1\n",
    "list_SGD=gradient_for_lin_reg_MB(Y,X,B)\n",
    "print(\"a={a} , b={b}\".format(a=list_SGD[0],b=list_SGD[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccfe8b",
   "metadata": {},
   "source": [
    "$$\\newline $$\n",
    "\n",
    "\n",
    "For the learning_rate, batch size and number of iterations I have used:\n",
    "\n",
    "__The order of execution time:__ $simple-gradient-descent < stochastic-gradient-descent << minibatch-gradient-descent(batch-size=150)$\n",
    "\n",
    "__Accuracy:__ $ $ \n",
    "$minibatch-gradient-descent \\approx simple-gradient-descent > stochastic-gradient-descent$            i.e. minibatchSGD and simple-gradient-descent produce nearly same values of a and b which are approximately accurate and stochastic-gradient descent's results are not accurate\n",
    "\n",
    "$$\\newline $$\n",
    "$$\\newline $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b864ed",
   "metadata": {},
   "source": [
    "__Problem with stochastic gradient descent(SGD):__ It can be seen that the final values a and b produced by SGD, for the same values of learning-rate and number of iterations as simple gradient-descent, have high variance. This is because SGD uses gradient of only one training data and this data may be noisy.\n",
    "This problem doesn't occur in minibatch SGD because average gradient of multiple training data is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac089382",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$ $$\n",
    "The important advantage of stochastic gradient descent ( both SGD and minibatch SGD ) is that __SGD doesn't get stuck at local optima due to bad initial value of paramter, unlike simple gradient descent.__\n",
    "Simple gradient descent stucks at local optima because when it reaches there, the value of gradient becomes very small and so parameter doesn't change very much. But stochastic gradient descent doesn't compute gradient according to current situation but rather it randomly chooses a point and computes gradient value on it. So even if we are near to local optima, the parameter value can change drastically and hence move us to a far away point.\n",
    "\n",
    "But also note that, it is also possible that SGD will not take us out of local optima.Also, nny training data can be choosen by it to compute our gradient. So, it is possible that SGD may take us out of global maxima whereas simple gradient descent may work fine. To reduce chances of this situation, minibatch SGD is used.\n",
    "\n",
    "$$ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ac3a9",
   "metadata": {},
   "source": [
    "$$ $$\n",
    "$\\normalsize \\textbf {Optimal minibatch size:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea21b8",
   "metadata": {},
   "source": [
    "To find optimal batch size(that produces accurate results), i will compute, for each batch size, the average of all values of a and the average of all values of b over different training data points. Then I will see which batch size produces the closest value of a and b to 0.3 and 2.0 respectively. But to make code faster, rather than computing batch size for each integer, I will do it for batch sizes in multiples of 50 i.e 50,100,150,...\n",
    "\n",
    "UPDATE: After implementing the above way, it was found that code was taking too long to run(more than a hour). So, I will find values of a and b for each batch and consider the optimum batch-size to be the one for which value of a and b are near to 0.3 and 2.0. Also, I will take batch sizes only as 2,4,8,...,2048 i.e. powers of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "c270035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe_MB=pd.DataFrame(columns=['B','a','b'])   # a dataframe which stores corresponding batch-sizes and the values of a and b\n",
    "list=(2,4,16,32,64,128,256,512,1024,2048)   #batch sizes\n",
    "    \n",
    "    \n",
    "#generating different dataset\n",
    "np.random.seed(0)\n",
    "X=2.5*np.random.randn(10000) + 1.5  # array of 10000 values with mean 1.5 and standard deviation 2.5\n",
    "res=1.5*np.random.randn(10000)      # array of 10000 values with mean 0 and standard deviation 1.5\n",
    "Y=2 + 0.3*X + res                   # actual values of Y for above generated X\n",
    "    \n",
    "for j in list:          \n",
    "    B=j                             #batch-size update\n",
    "    list_optimal_batch_size=gradient_for_lin_reg_MB(Y,X,B)\n",
    "    dataframe_MB=dataframe_MB.append({'B':B,'a':list_optimal_batch_size[0],'b':list_optimal_batch_size[1]},ignore_index=True)\n",
    "        \n",
    "        \n",
    "#dataframe_MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "aae9b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.0\n"
     ]
    }
   ],
   "source": [
    "#finding batch size for which a and b are closest to 0.3 and 2.0\n",
    "min_distance=np.abs(0.3-dataframe_MB['a'].iloc[0])+np.abs(2.0-dataframe_MB['b'].iloc[0])  # absolute value of distances\n",
    "opt_B=0\n",
    "\n",
    "for i in range(len(dataframe_MB)):\n",
    "    if np.abs(0.3-dataframe_MB['a'].iloc[i])+np.abs(2.0-dataframe_MB['b'].iloc[i])<min_distance:\n",
    "        min_distance=np.abs(0.3-dataframe_MB['a'].iloc[i])+np.abs(2.0-dataframe_MB['b'].iloc[i])\n",
    "        opt_B=dataframe_MB['B'].iloc[i]\n",
    "        \n",
    "print(opt_B)     #optimum batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed5995",
   "metadata": {},
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eab6f3",
   "metadata": {},
   "source": [
    "### 2(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b2075",
   "metadata": {},
   "source": [
    "$$ \\normalsize\n",
    "P(cold \\cap fever)\n",
    "=P(fever|cold)*P(cold)\n",
    "$$\n",
    "$$\\normalsize=(0.307)*(0.02)$$\n",
    "$$\\normalsize=0.00614$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa34fd",
   "metadata": {},
   "source": [
    "### 2(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b760c5",
   "metadata": {},
   "source": [
    "Let lung disease be LD\n",
    "\n",
    "$$ \\normalsize P(cold | cough)  =\\frac{P(cough| cold)*P(cold)}{P(cough)}     \\qquad -->eqn1  $$\n",
    "$$ $$\n",
    "\n",
    "$$\\normalsize P(cough)= P(cough | LD \\cap cold)* P(LD \\cap cold) + P(cough | LD \\cap \\overline {cold})* P(LD \\cap \\overline {cold}) + P(cough | \\overline {LD} \\cap cold)* P(\\overline {LD} \\cap cold) + P(cough | \\overline {LD} \\cap \\overline {cold})* P(\\overline {LD} \\cap \\overline {cold})  \\qquad -->eqn(2) \\newline $$\n",
    "$$ \\newline$$\n",
    "\n",
    "From the given bayesian network, it is clear that lung disease and cold are independent events. Moreover, if A and B are independent then so are $\\overline A$ and B , A and $\\overline B$ and $\\overline A$ and $\\overline B $ $  \\newline$\n",
    "\n",
    "From eqn (2):-\n",
    "$$\\normalsize P(cough)=(0.7525)*P(LD)*P(cough)+ (0.505)*P(LD)*P(\\overline {cough}) + (0.505)*P(\\overline {LD})*P(cough) + (0.01)*P(\\overline{LD}) *P(\\overline{cough})  \\qquad -->eqn(3)$$\n",
    "\n",
    "$$ \\newline$$\n",
    "\n",
    "Finding the value of P(LD):\n",
    "\n",
    "$$\\normalsize P(LD)=P(L|snokes)*P(smokes)+P(LD|\\overline{smokes})*P(smokes)$$\n",
    "$$\\normalsize =(0.1009)*(0.2) + (0.001)*(0.8)$$\n",
    "$$\\normalsize=0.02018+0.0008$$\n",
    "$$\\normalsize=0.02098$$\n",
    "\n",
    "Putting this P(LD) in eqn(3):\n",
    "\n",
    "$$\\normalsize P(cough) = 0.7525*0.02098*0.02 \\quad + \\quad 0.505*0.02098*0.98 \\quad + \\quad 0.505*0.97902*0.02 \\quad + \\quad 0.01*0.97902*0.98$$\n",
    "$$\\normalsize=0.03018$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\normalsize P(cough|cold)=P(cough| cold \\cap LD)*P(LD) \\quad + \\quad P(cough|cold \\cap \\overline{LD})*P(\\overline{LD})$$\n",
    "\n",
    "$$\\normalsize = 0.7525*0.02098 \\quad + \\quad 0.505*0.97902$$\n",
    "$$\\normalsize=0.51019$$\n",
    "\n",
    "\n",
    "From eqn(1):-\n",
    "\n",
    "$$\\normalsize P(cold | cough)= \\frac{0.51019*0.02}{0.03018}$$\n",
    "\n",
    "$$\\normalsize=0.33809$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f737e3",
   "metadata": {},
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b8722",
   "metadata": {},
   "source": [
    "The probability mass function (pmf) of multinomial distribution is given by:-\n",
    "$$P= {n \\choose {x_1,x_2,..,x_k}} ({p_1}^{x_1}) *({p_2}^{x_2}) .... ({p_k}^{x_k})$$\n",
    "$$\\newline$$\n",
    "__Maximum likelihood estimate for multinomial distribution__\n",
    "In multinomial distribution, we perform n number of trials and each trial can result in either of these k possible outcomes:- $n_1 , n_2 , n_3 ,...., n_k$. \n",
    "Assume it has been observed that $n_1,n_2,...,n_k$ occured $x_1,x_2,...,x_k$ times and they have probabilities $p_1,p_2,...,p_k$ respectively. Then:-\n",
    "\n",
    "$$f(x_1,x_2,...x_k | p_1,p_2,...p_k)={n \\choose {x_1,x_2,..,x_k}} ({p_1}^{x_1}) *({p_2}^{x_2}) .... ({p_k}^{x_k})$$\n",
    "\n",
    "where, $$\\sum_{i=1}^k p_i =1 \\quad and \\quad n=\\sum_{i=1}^k x_i$$\n",
    "      $$\\newline$$\n",
    "      \n",
    "This function f tells us the probability that $n_1,n_2,...,n_k$ occurs $x_1,x_2,...,x_k$ times respectively given that each has probability $p_1,p_2,...,p_k$ respectively.\n",
    "$$\\newline$$\n",
    "Let $lik(p_1,p_2,...,p_k)$ denote the likelihood of $p_1,p_2,...p_k$. We have to maximise this likelihood.\n",
    "\n",
    "I will maximise the log of likelihood. Since log is monotonically increasing function, there will be no difference from what we want.\n",
    "\n",
    "__Note:__ there is a constraint here viz. $\\sum_{i=1}^k p_i =1$. So I will use lagrange's multiplier. \n",
    "$$\\newline$$\n",
    "$$lik(p_1,p_2,...,p_k,\\lambda)=log(f(x_1,x_2,...x_k | p_1,p_2,...p_k))+ \\lambda (1-\\sum_{i=1}^k p_i)$$\n",
    "$$=log(n!)- \\sum_{i=1}^k log x_i + \\sum_{i=1}^k x_i log p_i + \\lambda (1-\\sum_{i=1}^k p_i)$$\n",
    "\n",
    "$$\\newline$$\n",
    "\n",
    "Differentiating wrt $p_i$ and $\\lambda$ alsoand equating to 0 we will get the optimum value of each $p_i$ such that likelihood will be maximised:-\n",
    "\n",
    "$$\\frac{\\partial lik}{\\partial p_i}=0$$\n",
    "$$\\frac{\\partial \\sum_{i=1}^k x_i log p_i + \\lambda (1-\\sum_{i=1}^k p_i)}{\\partial p_i} =0$$\n",
    "\n",
    "$$\\newline$$\n",
    "\n",
    "This gives $\\lambda p_i =x_i$ and $\\lambda=n$\n",
    "\n",
    "So, $p_i$= $\\frac{x_i}{n}$  for $i=1,2,...,k$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
